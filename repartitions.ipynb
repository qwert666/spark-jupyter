{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitions in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import datetime\n",
    "from pyspark.sql.functions import to_date, col, spark_partition_id, monotonically_increasing_id, rand\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Repartition\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = spark.createDataFrame([{\"foo\": random.randint(0, 10)} for x in range(0, 1000)])\n",
    "t.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default partitioning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|foo|\n",
      "+---+\n",
      "|  4|\n",
      "| 10|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[125, 125, 125, 125, 125, 125, 125, 125]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.write.format('parquet').save(\"./partition_data/default_partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       8\n"
     ]
    }
   ],
   "source": [
    "!ls ./partition_data/default_partition | grep part | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition with a column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|foo|\n",
      "+---+\n",
      "|  0|\n",
      "|  0|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_rep = t.repartition(\"foo\")\n",
    "t_rep.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 90, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_rep.rdd.glom().map(len).collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rep.write.format('parquet').save('./partition_data/with_col_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      12\n"
     ]
    }
   ],
   "source": [
    "!ls ./partition_data/with_col_name | grep part | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition with number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = spark.createDataFrame([{\"foo\": random.randint(0, 10)} for x in range(0, 1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_num = t.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|foo|\n",
      "+---+\n",
      "|  3|\n",
      "|  3|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_num.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_num.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[504, 496]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_num.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_num.write.format('parquet').save('./partition_data/with_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       2\n"
     ]
    }
   ],
   "source": [
    "!ls ./partition_data/with_size | grep part | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition with number and column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|foo|\n",
      "+---+\n",
      "|  4|\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_rep_size = t.repartition(2, \"foo\")\n",
    "t_rep_size.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_rep_size.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[341, 659]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_rep_size.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rep_size.write.format('parquet').save('./partition_data/with_col_name_and_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       2\n"
     ]
    }
   ],
   "source": [
    "!ls ./partition_data/with_col_name_and_size | grep part | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition with number, column name and random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|foo|\n",
      "+---+\n",
      "|  8|\n",
      "| 10|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_rep_rand = t.repartition(2, \"foo\", rand())\n",
    "t_rep_rand.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[480, 520]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_rep_rand.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rep_rand.write.format('parquet').save('./partition_data/with_col_name_and_size_and_rand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       2\n"
     ]
    }
   ],
   "source": [
    "!ls -R ./partition_data/with_col_name_and_size_and_rand | grep part | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RepartitionBy on Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.select(\"foo\", monotonically_increasing_id().alias('id')).write.format(\"parquet\").partitionBy(\"foo\").save('./partition_data/with_writer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS \u001b[1m\u001b[36mfoo=1\u001b[m\u001b[m    \u001b[1m\u001b[36mfoo=2\u001b[m\u001b[m    \u001b[1m\u001b[36mfoo=4\u001b[m\u001b[m    \u001b[1m\u001b[36mfoo=6\u001b[m\u001b[m    \u001b[1m\u001b[36mfoo=8\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mfoo=0\u001b[m\u001b[m    \u001b[1m\u001b[36mfoo=10\u001b[m\u001b[m   \u001b[1m\u001b[36mfoo=3\u001b[m\u001b[m    \u001b[1m\u001b[36mfoo=5\u001b[m\u001b[m    \u001b[1m\u001b[36mfoo=7\u001b[m\u001b[m    \u001b[1m\u001b[36mfoo=9\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls ./partition_data/with_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      99\n"
     ]
    }
   ],
   "source": [
    "!ls -R ./partition_data/with_writer/ | grep part | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = spark.read.format(\"parquet\").load('./partition_data/with_writer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('foo = 1)\n",
      "+- Relation[id#132L,foo#133] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, foo: int\n",
      "Filter (foo#133 = 1)\n",
      "+- Relation[id#132L,foo#133] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(foo#133) && (foo#133 = 1))\n",
      "+- Relation[id#132L,foo#133] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [id#132L,foo#133] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/mdatberg/Codez/Springer-Media/media_impact_bucket_testing/jupyter/p..., PartitionCount: 1, PartitionFilters: [isnotnull(foo#133), (foo#133 = 1)], PushedFilters: [], ReadSchema: struct<id:bigint>\n"
     ]
    }
   ],
   "source": [
    "r.filter(\"foo = 1\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = spark.read.format(\"parquet\").load('./partition_data/with_col_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('foo = 1)\n",
      "+- Relation[foo#136L] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "foo: bigint\n",
      "Filter (foo#136L = cast(1 as bigint))\n",
      "+- Relation[foo#136L] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(foo#136L) && (foo#136L = 1))\n",
      "+- Relation[foo#136L] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [foo#136L]\n",
      "+- *(1) Filter (isnotnull(foo#136L) && (foo#136L = 1))\n",
      "   +- *(1) FileScan parquet [foo#136L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/mdatberg/Codez/Springer-Media/media_impact_bucket_testing/jupyter/p..., PartitionFilters: [], PushedFilters: [IsNotNull(foo), EqualTo(foo,1)], ReadSchema: struct<foo:bigint>\n"
     ]
    }
   ],
   "source": [
    "r2.filter(\"foo = 1\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
